{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os, random\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy import interpolate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "np.random.seed( 0 )\n",
    "sns.set_style( \"darkgrid\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('C:/Users/lselig/selig-fa/finance/.data/fundamental_analysis_neglog.parquet')\n",
    "df = df[(df.period == \"Q2\") & (df.calendarYear == \"2022\")]\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "df.dropna(axis=0, inplace = True)\n",
    "df = df.drop_duplicates()\n",
    "targets = df.wk_curr_to_next_pct_inc.values\n",
    "\n",
    "targets_binary = targets >= 1.0 # bools\n",
    "targets_binary = np.array(targets_binary, dtype = \"int\")\n",
    "plt.close()\n",
    "plt.hist(targets_binary)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(targets_binary)\n",
    "tickers = df.symbol.values\n",
    "periods = df.period.values\n",
    "calendarYears = df.calendarYear.values\n",
    "date = df.date.values\n",
    "qualitative_data = np.array((tickers, periods, calendarYears, date)).T\n",
    "df = df.drop(columns = [\"period\", \"calendarYear\", \"symbol\", \"date\", \"open_next\", \"wk_curr_to_next_pct_inc\"])\n",
    "features = df.to_numpy()\n",
    "print(targets.shape, features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features =  scaler.fit_transform(features)\n",
    "features = np.clip(features, -5, 5)\n",
    "print(features.shape)\n",
    "n_features = features.shape[1]\n",
    "print(n_features)\n",
    "%matplotlib widget \n",
    "for i in range(n_features):\n",
    "    print(i, np.nanmin(features[:, i]), np.nanmax(features[:, i]))\n",
    "    sns.kdeplot(features[:, i])\n",
    "plt.title(\"Distribution of final features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df))\n",
    "print(features.shape, targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(features.T)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "# print(pca.singular_values_)\n",
    "# print(components)\n",
    "# plt.hist2d(components[:, 0], components[:, 1], bins = 50)\n",
    "components = pca.components_.T\n",
    "print(components.shape)\n",
    "# components[0,:]\n",
    "plt.close()\n",
    "plt.hist2d(components[:, 0], components[:, 1], bins = 60)\n",
    "plt.show()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "clst = KMeans(n_clusters=3, random_state=0).fit(components)\n",
    "# clst = AffinityPropagation( random_state=0).fit(components)\n",
    "labels = clst.labels_.reshape(-1, 1)\n",
    "pca_and_labels = np.hstack((components, labels))\n",
    "print(pca_and_labels.shape)\n",
    "clst.labels_.shape\n",
    "fig, axs = plt.subplots(1, 2, figsize = (12, 8))\n",
    "for g in np.unique(labels):\n",
    "    idxs = np.where(pca_and_labels[:, 2] == g)[0]\n",
    "    cluster = pca_and_labels[idxs]\n",
    "    for i, c in enumerate(cluster):\n",
    "        print(c[0], c[1], qualitative_data[:, 0][idxs[i]])\n",
    "        axs[0].text(c[0], c[1], s = qualitative_data[:, 0][idxs[i]])\n",
    "    axs[0].scatter(cluster[:, 0],  cluster[:, 1], label = g)\n",
    "axs[0].set_xlabel(\"PCA1\")\n",
    "axs[0].set_ylabel(\"PCA2\")\n",
    "axs[1].hist2d(pca_and_labels[:, 0], pca_and_labels[:, 1], bins = 40)\n",
    "axs[1].set_xlabel(\"PCA1\")\n",
    "axs[1].set_ylabel(\"PCA2\")\n",
    "axs[0].legend()\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "# plt.scatter(pca_and_labels[:, 0], pca_and_labels[: ,1], group = )\n",
    "# plt.close()\n",
    "# plt.hist(kmeans.labels_)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tickers where PCA2 is an outlier are all related to Federal National Mortgage Association (FNMA)\n",
    "# not sure if it can be exploited\n",
    "interesting = np.argwhere(pca_and_labels[: ,1] >= 0.08)\n",
    "interesting_tickers = tickers[interesting]\n",
    "# print(interesting_tickers)\n",
    "\n",
    "interesting = np.argwhere(pca_and_labels[: ,1] <= -0.0365)\n",
    "interesting_tickers = tickers[interesting]\n",
    "print(interesting_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "interesting = np.argwhere(pca_and_labels[: ,0] >= 0.0055)\n",
    "interesting_tickers = qualitative_data[interesting]\n",
    "print(interesting_tickers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "plt.close()\n",
    "# good = np.where(targets <= 100)[0]\n",
    "corrs = [] \n",
    "for i in range(n_features):\n",
    "    f = features[:, i]\n",
    "    t = targets\n",
    "    r = np.corrcoef(f, t)[0, 1]\n",
    "    corrs.append(r)\n",
    "\n",
    "sns.kdeplot(corrs)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size = 0.2, random_state = 42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_train)\n",
    "print(mean_absolute_error(y_pred, y_train))\n",
    "print(r2_score(y_pred, y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 ... 1 1 0]\n",
      "[0 1 0 ... 0 0 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.80      0.07         5\n",
      "           1       0.99      0.61      0.76       279\n",
      "\n",
      "    accuracy                           0.61       284\n",
      "   macro avg       0.51      0.70      0.41       284\n",
      "weighted avg       0.98      0.61      0.74       284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "print(targets_binary)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets_binary, test_size = 0.2, random_state = 42)\n",
    "print(y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(y_pred)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87f4a1bc75633a3ce313b337f8412d9265ee8b5e5a21bcd5bca49bf858097b07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
